# Introduction

Welcome to this repository where I share some of the most influential papers in the field of machine learning. This repository includes seminal works on various aspects of these topicss. Each paper listed has contributed significantly to advancing our understanding and capabilities in machine learning. Feel free to explore these papers to deepen your knowledge and stay updated with the latest advancements in this exciting field.

### Optimization Problem with Gradient Descent

* [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)
* [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836)
* [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962)
* [Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes](https://arxiv.org/abs/1711.04325)
* [Stochastic Weight Averaging in Parallel: Large-Batch Training that Generalizes Well](https://arxiv.org/abs/2001.02312)
* [Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)
* [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)

### Adaptive Learning Rate

* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265)

### Normalization

* [How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)
* [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models](https://arxiv.org/abs/1702.03275)
* [Layer Normalization](https://arxiv.org/abs/1607.06450)
* [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)
* [Group Normalization](https://arxiv.org/abs/1803.08494)
* [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks](https://arxiv.org/abs/1602.07868)
* [Spectral Norm Regularization for Improving the Generalizability of Deep Learning](https://arxiv.org/abs/1705.10941)

### Convolutional Neural Network

* [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf)

### "Deep" Learning

* [Conversational Speech Transcription Using Context-Dependent Deep Neural Networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf)

### Spatial Transformer Layer

* [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

### Large Language Model

* [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)
* [An In-depth Look at Gemini's Language Abilities](https://arxiv.org/abs/2312.11444)
* [Examining Forgetting in Continual Pre-training of Aligned Large Language Models](https://arxiv.org/abs/2401.03129)
* [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
* [Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model](https://arxiv.org/abs/1909.09587)
* [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)
* [Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning](https://arxiv.org/abs/2206.03931)
* [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)
* [Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://arxiv.org/abs/2210.06774)
* [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
* [DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents](https://arxiv.org/abs/2303.17071)
* [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207)
* [Inner Monologue: Embodied Reasoning through Planning with Language Models](https://arxiv.org/abs/2207.05608)

### Prompt Engineering

* [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910)
* [Can Large Language Models Be an Alternative to Human Evaluations?](https://arxiv.org/abs/2305.01937)
* [A Closer Look into Automatic Evaluation Using Large Language Models](https://arxiv.org/abs/2310.05657)
* [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760)
* [Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/abs/2312.16171)
* [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401)

### Reinforcement Learning

* [Learning to Generate Prompts for Dialogue Generation through Reinforcement Learning](https://arxiv.org/abs/2206.03931)
* [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)

### In-context Learning

* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
* [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)
* [Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)

### Chain of Thoughts

* [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
* [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)
